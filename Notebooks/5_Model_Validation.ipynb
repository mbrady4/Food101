{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model performance on test dataset\n",
    "\n",
    "In this notebook, we will explore the model's performance on the test dataset. In particular we will focus on accuracy and demonstrate the ability to boost accuracy performance using test time augmentation. \n",
    "\n",
    "\n",
    "As noted in the prior notebook, our model is performing at >90% (~93%) top 5 accuracy. This notebook will demonstrate that the model, with test time augmentation, can classify images from the food-101 test dataset with 85.3% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the model saved as the output of the prior notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('fully_trained_resnet.h5', compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells creates a data pipeline for the test dataset. At this stage we do not apply image augmentations, but simply rescale the image. We also turn shuffle off to ensure our labels align with the data passed into the model. We also set the batch_size to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25250 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "samples = datagen.flow_from_directory('/home/jupyter/data/test',\n",
    "                                      class_mode='categorical',\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      target_size=(512,512))\n",
    "samples.shuffle = False\n",
    "samples.reset()\n",
    "nums_labels = samples.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate predictions for each image. As the batch size is one, we need to take a step for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict_generator(samples, steps=25250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the predictions in the correct format, we take the max number from each prediction array. The model outputs an array with a predicted probability for each of the 101 classes, at this stage we just want the class with the highest predicted probability. \n",
    "\n",
    "We also have to subtract one from the classes because as the provided begin with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   8, ..., 100, 100, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.argmax(axis=1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 100, 100, 100], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the accuracy_score function from sci-kit learn to evaluate the accuracy performance of our model. As shown below, the model performs with 83.3% accuracy on the test dataset. Not bad, but short of our goal to achieve greater than 85% accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8335841584158415"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, test_preds.argmax(axis=1)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using test time augmentation to boost performance to >85% top 1 accuracy\n",
    "\n",
    "Test time augmentation uses image augmentation techniques traditionally used during training when making predictions. This means that rather than having our model make a single prediction based on an image, we have the model make multiple predictions of the same image (with the image augmented slightly differently each time). We than take the average of all the predictions to arrive at a final prediction. \n",
    "\n",
    "As shown below, this approach can substantially boost performance. In our case it boosts top-1 accuracy form 83.3% to 85.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepeating the data pipeline for TTA is identical to the pipeline shown above (included here for reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25250 images belonging to 101 classes.\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for TTA\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "samples = datagen.flow_from_directory('/home/jupyter/data/test',\n",
    "                                      class_mode='categorical',\n",
    "                                      batch_size=1,\n",
    "                                      shuffle=False,\n",
    "                                      target_size=(512,512))\n",
    "samples.shuffle = False\n",
    "samples.reset()\n",
    "labels = samples.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function, accepts a datagenerator object, a model, an image, and a specified number of test time augmentations to make. The function returns predictions for each augmentation as well as the combined prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_prediction(datagen, model, image, n_examples=7):\n",
    "    augmenter = datagen.flow(image[0], batch_size=1)\n",
    "    augmenter.shuffle = False\n",
    "    augmenter.reset()\n",
    "    preds = model.predict_generator(augmenter, steps=n_examples, verbose=0)\n",
    "    all_preds = np.mean(preds, axis=0)\n",
    "    pred = np.argmax(all_preds, axis=-1)\n",
    "    return pred, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function accepts a model, array of images, and an array of labels. The function takes each image and generates a prediction that utilizes test time augmentation (11 predictions per image) and the generates a combined prediction. The function returns the label array, the combined prediction array, and an array with all of the predictions.\n",
    "\n",
    "Note, the y_label array is not strictly necessary to this function at this time, but could be used in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_augmentation(model, X_samples, y_labels):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                 brightness_range=[0.7,1.3],\n",
    "                                 rotation_range=30,\n",
    "                                 horizontal_flip=True,\n",
    "                                 zoom_range=[0.7,1.3],\n",
    "                                 fill_mode='nearest')\n",
    "    examples_per_image = 11\n",
    "    preds = []\n",
    "    all_preds = []\n",
    "    for i in range(len(X_samples)):\n",
    "        if ((i % 500) == 0):\n",
    "            print(i, 'Predictions Complete')\n",
    "        pred, all_preds = test_time_prediction(datagen, model, X_samples[i], examples_per_image)\n",
    "        preds.append(pred)\n",
    "    return y_labels, preds, all_preds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell generates predictions utilizing test time augmentation for every image in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Predictions Complete\n",
      "500 Predictions Complete\n",
      "1000 Predictions Complete\n",
      "1500 Predictions Complete\n",
      "2000 Predictions Complete\n",
      "2500 Predictions Complete\n",
      "3000 Predictions Complete\n",
      "3500 Predictions Complete\n",
      "4000 Predictions Complete\n",
      "4500 Predictions Complete\n",
      "5000 Predictions Complete\n",
      "5500 Predictions Complete\n",
      "6000 Predictions Complete\n",
      "6500 Predictions Complete\n",
      "7000 Predictions Complete\n",
      "7500 Predictions Complete\n",
      "8000 Predictions Complete\n",
      "8500 Predictions Complete\n",
      "9000 Predictions Complete\n",
      "9500 Predictions Complete\n",
      "10000 Predictions Complete\n",
      "10500 Predictions Complete\n",
      "11000 Predictions Complete\n",
      "11500 Predictions Complete\n",
      "12000 Predictions Complete\n",
      "12500 Predictions Complete\n",
      "13000 Predictions Complete\n",
      "13500 Predictions Complete\n",
      "14000 Predictions Complete\n",
      "14500 Predictions Complete\n",
      "15000 Predictions Complete\n",
      "15500 Predictions Complete\n",
      "16000 Predictions Complete\n",
      "16500 Predictions Complete\n",
      "17000 Predictions Complete\n",
      "17500 Predictions Complete\n",
      "18000 Predictions Complete\n",
      "18500 Predictions Complete\n",
      "19000 Predictions Complete\n",
      "19500 Predictions Complete\n",
      "20000 Predictions Complete\n",
      "20500 Predictions Complete\n",
      "21000 Predictions Complete\n",
      "21500 Predictions Complete\n",
      "22000 Predictions Complete\n",
      "22500 Predictions Complete\n",
      "23000 Predictions Complete\n",
      "23500 Predictions Complete\n",
      "24000 Predictions Complete\n",
      "24500 Predictions Complete\n",
      "25000 Predictions Complete\n"
     ]
    }
   ],
   "source": [
    "y_labels, preds, all_preds = test_time_augmentation(model, samples, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells demonstrate what the first 10 predictions are versus the labels. As expected, since shuffle is turned off, all the first ten images are from the first class. Note, as discussed previously, the labels begin counting at 0; thus we add one to each label at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 22, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels[:10] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, with test time augmentation the performance of the image classifier on the food-101 test set exceeds 85% accuracy, with performance of 85.3% top-1 accuracy.\n",
    "\n",
    "This notebook demonstrates the power of test time augmentation as a technique to boost performance of predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8533465346534653\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_labels+1, preds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cells save the combined prediction, label, all prediction arrays as csv files to enable future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.asarray(preds)\n",
    "np.savetxt('preds.csv', preds, delimiter=',')\n",
    "\n",
    "y_labels = np.asarray(y_labels)\n",
    "np.savetxt('y_labels.csv', y_labels, delimiter=',')\n",
    "\n",
    "np.savetxt('all_preds.csv', all_preds, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
