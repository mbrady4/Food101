{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Mini Train and Test Sets for Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/brady/Documents/GitHub/food101\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the dataset with 12 classes instead of 101\n",
    "import os\n",
    "\n",
    "base_dir = '/Users/brady/Documents/GitHub/food101/data'\n",
    "min_train_dir = base_dir + '/min_train'\n",
    "min_test_dir = base_dir + '/min_test'\n",
    "\n",
    "# only run once\n",
    "os.mkdir(min_train_dir)\n",
    "os.mkdir(min_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_dict = json.load(open('food-101/meta/train.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_dir = original_dataset_dir = '/Users/brady/Documents/GitHub/food101/food-101/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 35, 17, 13, 72, 58, 77, 17, 82, 0, 8, 82]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_nums = []\n",
    "for x in range(12):\n",
    "  random_nums.append(random.randint(0,100))\n",
    "random_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text = pd.read_csv('/Users/brady/Documents/GitHub/food101/food-101/meta/classes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot_and_sour_soup',\n",
       " 'falafel',\n",
       " 'chicken_curry',\n",
       " 'carrot_cake',\n",
       " 'panna_cotta',\n",
       " 'lasagna',\n",
       " 'poutine',\n",
       " 'chicken_curry',\n",
       " 'red_velvet_cake',\n",
       " 'baby_back_ribs',\n",
       " 'breakfast_burrito',\n",
       " 'red_velvet_cake']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_classes = []\n",
    "for num in random_nums:\n",
    "    random_classes.append(text.values[num][0])\n",
    "random_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for key in train_dict.keys():\n",
    "    if key in random_classes:\n",
    "        \n",
    "        # Create class directors in min_train folder\n",
    "        class_dir = os.path.join(min_train_dir, key)\n",
    "        os.mkdir(class_dir)\n",
    "        \n",
    "        # Copy images to min_train folder\n",
    "        for file in train_dict[key]:\n",
    "            src = os.path.join(original_dataset_dir, file + '.jpg')\n",
    "            dst = os.path.join(min_train_dir, file + '.jpg')\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = json.load(open('food-101/meta/test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in test_dict.keys():\n",
    "    if key in random_classes:\n",
    "        \n",
    "        # Create class directors in min_train folder\n",
    "        class_dir = os.path.join(min_test_dir, key)\n",
    "        os.mkdir(class_dir)\n",
    "        \n",
    "        # Copy images to min_train folder\n",
    "        for file in test_dict[key]:\n",
    "            src = os.path.join(original_dataset_dir, file + '.jpg')\n",
    "            dst = os.path.join(min_test_dir, file + '.jpg')\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Base Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(parameters):\n",
    "    train_datagen = ImageDataGenerator(rescale=parameters['rescale'],\n",
    "                                       brightness_range=parameters['brightness_range'],\n",
    "                                       rotation_range=parameters['rotation_range'],\n",
    "                                       zoom_range=parameters['zoom_range'],\n",
    "                                       fill_mode=parameters['fill_mode'])\n",
    "    val_datagen = ImageDataGenerator(rescale=parameters['rescale'])\n",
    "    \n",
    "    train_datagen = train_datagen.flow_from_directory('data/min_train/', seed=42, class_mode='categorical', target_size=parameters['target_size'])\n",
    "    val_datagen = val_datagen.flow_from_directory('data/min_test/', seed=42, class_mode='categorical', target_size=parameters['target_size'])\n",
    "    \n",
    "    train_steps = len(train_datagen)\n",
    "    val_steps = len(val_datagen)\n",
    "    classes = len(list(train_datagen.class_indices.keys()))\n",
    "    \n",
    "    conv_base = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=parameters['input_shape'])\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(conv_base)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(parameters['dropout_1']))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(parameters['dropout_2']))\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    \n",
    "    conv_base.trainable = False\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(lr=parameters['lr']),\n",
    "              metrics=['acc','top_k_categorical_accuracy'])\n",
    "    \n",
    "    callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=2,\n",
    "        )\n",
    "    ]\n",
    "        \n",
    "    history = model.fit_generator(\n",
    "        train_datagen,\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=parameters['epochs'],\n",
    "        verbose=2,\n",
    "        validation_data=val_datagen,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "def get_param_combos(parameters):\n",
    "    '''\n",
    "    parameters must be a dictionary\n",
    "    '''\n",
    "    combinations = it.product(*(parameters[parameter] for parameter in parameters.keys()))\n",
    "    final_combos = []\n",
    "    for combo in combinations:\n",
    "        temp = {}\n",
    "        for item in zip(parameters.keys(), combo):\n",
    "            temp[item[0]] = item[1]\n",
    "        final_combos.append(temp)\n",
    "    return final_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(rescale=[1./255], \n",
    "               brightness_range=[[0.8,1.2],[0.9,1.1]],\n",
    "               rotation_range=[0],\n",
    "               zoom_range=[1.0],\n",
    "               fill_mode=['nearest'],\n",
    "               epochs=[5], \n",
    "               lr=[0.001], \n",
    "               dropout_1=[0.5], \n",
    "               dropout_2=[0.5],\n",
    "               input_shape=[(256,256,3)],\n",
    "               target_size=[(256,256)]):\n",
    "        parameters = {'rescale':rescale,\n",
    "                      'brightness_range':brightness_range,\n",
    "                      'rotation_range':rotation_range,\n",
    "                      'zoom_range':zoom_range,\n",
    "                      'fill_mode':fill_mode,\n",
    "                      'epochs':epochs,\n",
    "                      'lr':lr,\n",
    "                      'dropout_1':dropout_1,\n",
    "                      'dropout_2':dropout_2,\n",
    "                      'input_shape':input_shape,\n",
    "                      'target_size':target_size\n",
    "                      }\n",
    "        combos = get_param_combos(parameters)\n",
    "        results = []\n",
    "        for combo in combos:\n",
    "            result = train_model(combo)\n",
    "            results.append(result)\n",
    "        return results, combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 images belonging to 10 classes.\n",
      "Found 2500 images belonging to 10 classes.\n",
      "WARNING:tensorflow:From /anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /anaconda3/envs/U4-S2-NNF/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "results, combos = tune_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NNF (Python 3)",
   "language": "python",
   "name": "u4-s2-nnf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
